## SpiderDaguo2.0——自定义过滤规则的精品文章爬取工具

### 背景
基于优质技术内容收集的初衷，半年前我写了一个爬取优质社区文章的爬虫，当时初学Node，各方面处理并不优雅，数据存储甚至用的还是非持久化的文件系统储存，最近用Node写了一个门户网站，缺少文章内容来填充，遂尝试重写代码，希望可以更优雅的实现多网站，多次爬取，健壮的爬虫系统。

### 初衷 
SpiderDaguo旨在帮不同身份的人提供更加符合预期的精品文章，剔除庞大信息中的糟粕，取其精华，节省时间的同时，使我们能保有最多的精力来阅读这些文章，更重要的是它提供可定制的文章筛选功能，你可以选择你喜欢的门户，要求的推送限制，比如至少要1000个赞的文章才收集，甚至推送时间，文章存储结构，匹配要求字段，一键搜索各大主流门户相关字眼的资讯干货，等等，当然这些功能都已经有了默认的主流选项，你可以只关心设置你想使用的功能。
后来发现已经有非常好的产品实现了我上面的一些想法了，即刻和轻芒杂志app甚至RSS都是优质内容获取的一些优秀方法，在这里我也极力向对知识，对各个领域有兴趣，对学习效率，阅读体验有更高体验要求的人群推荐这些应用服务。

### 原理 
项目主要基于Node的superagent模块，将要爬取的网站信息存储在sites目录之下，包括网站的基本信息和匹配的模板，分为列表页和文章详情页，用cheerio处理返回的文本内容，提取数据存储在MongoDB中，数据模型保存在models目录之下。

爬虫系统维护一个以网站名为粒度的任务列表，利用Promise异步依次执行，每一个网站首先爬取首页（列表页），将获取的列表链接封装成Promise任务，并发爬取所有链接获取文章，完成所有任务后触发下一个站点的爬取。

### LISENCE
MIT

